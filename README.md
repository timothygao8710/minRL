A zero-to-hero type repo in Deep RL, building up core algorithms (e.g., PPO, SAC, TD3) step-by-step, starting from basic definitions of policy gradient, Q-learning, etc. 

Each step introduces new techniques building on and motivated by the previous iteration, enabling solving harder and more general problems, faster, better, and more with more stability.

Through working on this repo, my goal is to understand these algorithms better.

The intended audience has read the SpinningUp docs, and wants to see the ideas built up step-by-step and the effect of each step verified emperically.

TODO: Finish Spinning Up Algos, Brax environments

<img width="1219" height="561" alt="Screenshot 2025-08-15 at 12 52 14 PM" src="https://github.com/user-attachments/assets/c052e9a1-b75b-4d94-b016-b8e9ffb04744" />

<img width="1212" height="559" alt="Screenshot 2025-08-15 at 12 51 34 PM" src="https://github.com/user-attachments/assets/f719240c-f03c-4af2-82e7-2f675520d1f8" />
